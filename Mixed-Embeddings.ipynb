{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Artist Embeddings Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import multiprocessing\n",
    "import pickle\n",
    "\n",
    "np.random.seed(0)\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "class ModelSaver(CallbackAny2Vec):\n",
    "    \"\"\"Gensim callback to save model every log_frequency epochs\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, d2v_object, rated_embeddings_path, w2v_model_path, log_frequency=5\n",
    "    ):\n",
    "        self.epoch = 1\n",
    "        self.log_frequency = log_frequency\n",
    "        self.d2v_object = d2v_object\n",
    "        self.rated_embeddings_path = rated_embeddings_path\n",
    "        self.w2v_model_path = w2v_model_path\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        if self.epoch % self.log_frequency == 0:\n",
    "            self.d2v_object.w2v_model = model\n",
    "            self.d2v_object.wv = model.wv\n",
    "            self.d2v_object.save_rated_vec(self.rated_embeddings_path)\n",
    "            self.d2v_object.save_w2v_model(self.w2v_model_path)\n",
    "        self.epoch += 1\n",
    "\n",
    "\n",
    "class D2V_Recommender:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size=100,\n",
    "        window=3,\n",
    "        min_count=1,\n",
    "        workers=multiprocessing.cpu_count() - 1,\n",
    "        num_epochs=50,\n",
    "        sample=0,  # do not downsample\n",
    "    ):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.num_epochs = num_epochs\n",
    "        self.sample = sample\n",
    "\n",
    "        self.wv = None\n",
    "        self.mean_embeddings = (\n",
    "            # access to embeddings with self.mean_embeddings[\"rater_user_id\"]\n",
    "            None\n",
    "        )\n",
    "        self.data_dict = None  # dict of arrays with X_train, X_test, y_train, y_test\n",
    "\n",
    "    def fit_rated_embeddings(\n",
    "        self, d2v_train, w2v_model_path, rated_embeddings_path, resume_training=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fit and save Word2Vec model to embed rated users based on dating behavior of raters.\n",
    "\n",
    "        :param d2v_train: A pd.Series of list of rated_ids (string) that were co-liked by a rater\n",
    "        \"\"\"\n",
    "        # Prepare the data iterator\n",
    "        d2v_train_iterator = self.build_data_iterator(d2v_train)\n",
    "\n",
    "        # Initiate the model\n",
    "        \n",
    "        model_saver = ModelSaver(self, rated_embeddings_path, w2v_model_path)\n",
    "        self.w2v_model = Word2Vec(\n",
    "            vector_size=self.embedding_size,\n",
    "            window=self.window,\n",
    "            min_count=self.min_count,\n",
    "            workers=self.workers,\n",
    "            sample=self.sample,\n",
    "            sg=1,  # skip-gram\n",
    "            hs=0,\n",
    "            negative=5,\n",
    "            callbacks=[model_saver],\n",
    "            seed=0,\n",
    "        )\n",
    "        model = self.w2v_model\n",
    "        if resume_training:\n",
    "            model = self.load_w2v_model(w2v_model_path)\n",
    "            model.build_vocab(d2v_train_iterator, update=True)\n",
    "        elif model.train_count == 0:\n",
    "            model.build_vocab(d2v_train_iterator)\n",
    "\n",
    "        # train and save final model\n",
    "        model.train(\n",
    "            d2v_train_iterator,\n",
    "            total_examples=model.corpus_count,\n",
    "            epochs=self.num_epochs,\n",
    "            compute_loss=True,\n",
    "        )\n",
    "\n",
    "        self.w2v_model = model\n",
    "        self.wv = model.wv\n",
    "        self.save_rated_vec(rated_embeddings_path)\n",
    "        self.save_w2v_model(w2v_model_path)\n",
    "\n",
    "    def build_data_iterator(self, data):\n",
    "        \"\"\" Create an iterator of which an iter is random passes on the data\"\"\"\n",
    "\n",
    "        class shuffle_generator:\n",
    "            def __init__(self, data):\n",
    "                self.data = data\n",
    "\n",
    "            def __iter__(self):\n",
    "                self.data.apply(np.random.shuffle)\n",
    "                return shuffle_generator_iter(self.data)\n",
    "\n",
    "        class shuffle_generator_iter:\n",
    "            def __init__(self, data):\n",
    "                self.i = 0\n",
    "                self.data = data\n",
    "                self.data_length = len(data)\n",
    "\n",
    "            def __iter__(self):\n",
    "                # Iterators are iterables too.\n",
    "                # Adding this functions to make them so.\n",
    "                return self\n",
    "\n",
    "            def __next__(self):\n",
    "                if self.i < 5 * self.data_length:\n",
    "                    # Shuffle at the end of the data\n",
    "                    if self.i % self.data_length == 0:\n",
    "                        self.data.apply(np.random.shuffle)\n",
    "                    i = self.i\n",
    "                    self.i += 1\n",
    "                    return self.data[i % self.data_length]  # a list\n",
    "                else:\n",
    "                    raise StopIteration()\n",
    "\n",
    "        return shuffle_generator(data)\n",
    "\n",
    "    def get_single_rated_vec(self, rated_id):\n",
    "        \"\"\" Get embedding vector of rated user of id rated_id\"\"\"\n",
    "        try:\n",
    "            return self.wv[str(rated_id)]\n",
    "        except KeyError:\n",
    "            # The rated user did not appear in the training dataset\n",
    "            return None\n",
    "\n",
    "    def get_single_rater_vec(self, rater_id):\n",
    "        \"\"\" Get embedding vector of rater user of id rater_id\"\"\"\n",
    "        try:\n",
    "            return self.mean_embeddings.loc[str(rater_id)].values\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    def save_rated_vec(self, wordvectors_path):\n",
    "        # wordvectors_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self.wv.save(str(wordvectors_path))\n",
    "\n",
    "    def load_rated_vec(self, wordvectors_path):\n",
    "        self.wv = KeyedVectors.load(str(wordvectors_path), mmap=\"r\")\n",
    "        return self.wv\n",
    "\n",
    "    def save_w2v_model(self, w2v_model_path):\n",
    "        # w2v_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self.w2v_model.save(str(w2v_model_path))\n",
    "\n",
    "    def load_w2v_model(self, w2v_model_path):\n",
    "        self.w2v_model = Word2Vec.load(str(w2v_model_path), mmap=\"r\")\n",
    "        return self.w2v_model\n",
    "\n",
    "    def save_data_dict(self, data_dict_path):\n",
    "        with open(data_dict_path, \"wb\") as handle:\n",
    "            pickle.dump(self.data_dict, handle,\n",
    "                        protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load_data_dict(self, data_dict_path):\n",
    "        with open(data_dict_path, \"rb\") as handle:\n",
    "            self.data_dict = pickle.load(handle)\n",
    "        return self.data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69411/69411 [00:00<00:00, 655630.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import AUC\n",
    "from keras import Model, Input\n",
    "from keras.layers import Embedding, concatenate, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "recommender = D2V_Recommender()\n",
    "recommender.load_rated_vec(\"rated.vectors\")\n",
    "max_rated_idx = 69563\n",
    "\n",
    "rated_id_to_emb_idx = {}\n",
    "rated_embedding_matrix = np.zeros(\n",
    "    (int(max_rated_idx) + 1, 100))\n",
    "    \n",
    "for user_id_str in tqdm(recommender.wv.key_to_index.keys()):\n",
    "    embedding_vector = recommender.wv[user_id_str]\n",
    "    if embedding_vector is not None:\n",
    "        user_id_int = int(user_id_str)\n",
    "        rated_embedding_matrix[user_id_int] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69564, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rated_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ClassificationModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ClassificationModel, self).__init__()\n",
    "\n",
    "    self.linear1 = torch.nn.Linear(8, 128)\n",
    "    self.relu1 = torch.nn.ReLU()\n",
    "    self.linear_final = torch.nn.Linear(128, 18)\n",
    "    \n",
    "  def forward(self, data):\n",
    "    data = data.float()\n",
    "    relu = self.relu1(self.linear1(data))\n",
    "    return relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_model = ClassificationModel()\n",
    "genre_model.load_state_dict(torch.load(\"genre_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = genre_model(torch.tensor([0.0030,0.724,0.989,0.3040,-5.922,0.1350,146.496,0.6930]))\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.embedding1 = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix), freeze=True)\n",
    "        self.embedding2 = ClassificationModel()\n",
    "        self.embedding2.load_state_dict(torch.load(\"genre_model.pt\"))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear1 = torch.nn.Linear(100+128, 32)\n",
    "        # self.relu2 = nn.ReLU()\n",
    "        # self.linear2 = torch.nn.Linear(128, 32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(32, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x,y):\n",
    "        data1 = self.embedding1(x)\n",
    "        data2 = self.embedding2(y)\n",
    "        data = torch.cat((data1.detach().squeeze(0), data2.detach().squeeze(0)), dim=0)\n",
    "        data = self.linear1(self.relu1(data))\n",
    "        # data = self.linear2(self.relu2(data))\n",
    "        data = self.linear3(self.relu3(data))\n",
    "        data = self.sigmoid(data)\n",
    "        return data\n",
    "\n",
    "model = EmbeddingLayer(rated_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0021])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model(torch.tensor([1]), torch.tensor([0.0030,0.724,0.989,0.3040,-5.922,0.1350,146.496,0.6930]))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.022009</td>\n",
       "      <td>0.2760</td>\n",
       "      <td>2.312927</td>\n",
       "      <td>0.4860</td>\n",
       "      <td>24827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>0.229626</td>\n",
       "      <td>0.0555</td>\n",
       "      <td>0.686852</td>\n",
       "      <td>0.3430</td>\n",
       "      <td>16188</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2990</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.7640</td>\n",
       "      <td>0.377583</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.335385</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>10042</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4630</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>-0.590614</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>-0.720394</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>47622</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>1.073389</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>-0.156486</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>27836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acousticness  danceability  energy  liveness  loudness  speechiness  \\\n",
       "0        0.1410         0.678   0.588    0.1370  0.022009       0.2760   \n",
       "1        0.0767         0.573   0.781    0.0813  0.229626       0.0555   \n",
       "2        0.2990         0.553   0.502    0.7640  0.377583       0.0409   \n",
       "3        0.4630         0.764   0.725    0.1090 -0.590614       0.0576   \n",
       "4        0.0509         0.697   0.865    0.2160  1.073389       0.0463   \n",
       "\n",
       "      tempo  valence  artist_id  target  \n",
       "0  2.312927   0.4860      24827       1  \n",
       "1  0.686852   0.3430      16188       1  \n",
       "2  0.335385   0.2960      10042       1  \n",
       "3 -0.720394   0.0794      47622       1  \n",
       "4 -0.156486   0.3220      27836       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"training_recommendation.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[80:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>1.213164</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>-0.933822</td>\n",
       "      <td>0.416</td>\n",
       "      <td>19989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.4070</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.2860</td>\n",
       "      <td>0.386446</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>-1.005683</td>\n",
       "      <td>0.577</td>\n",
       "      <td>51281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.536790</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.840950</td>\n",
       "      <td>0.147</td>\n",
       "      <td>51281</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.8950</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.667701</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.304184</td>\n",
       "      <td>0.274</td>\n",
       "      <td>37945</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.0609</td>\n",
       "      <td>0.880431</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>1.730278</td>\n",
       "      <td>0.384</td>\n",
       "      <td>52541</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.753952</td>\n",
       "      <td>0.0587</td>\n",
       "      <td>-1.278019</td>\n",
       "      <td>0.235</td>\n",
       "      <td>20414</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.518039</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.549449</td>\n",
       "      <td>0.639</td>\n",
       "      <td>29175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.1960</td>\n",
       "      <td>0.569858</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.828597</td>\n",
       "      <td>0.291</td>\n",
       "      <td>40507</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.9530</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.3190</td>\n",
       "      <td>-1.300739</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>-1.013342</td>\n",
       "      <td>0.491</td>\n",
       "      <td>63007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.4620</td>\n",
       "      <td>0.756679</td>\n",
       "      <td>0.2550</td>\n",
       "      <td>-0.931246</td>\n",
       "      <td>0.803</td>\n",
       "      <td>37480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     acousticness  danceability  energy  liveness  loudness  speechiness  \\\n",
       "90         0.0014         0.517   0.839    0.1080  1.213164       0.0287   \n",
       "92         0.4070         0.593   0.539    0.2860  0.386446       0.0349   \n",
       "94         0.1600         0.381   0.502    0.0842  0.536790       0.0368   \n",
       "95         0.8950         0.367   0.538    0.1490  0.667701       0.0421   \n",
       "96         0.2800         0.301   0.672    0.0609  0.880431       0.0623   \n",
       "..            ...           ...     ...       ...       ...          ...   \n",
       "321        0.0118         0.513   0.768    0.2940  0.753952       0.0587   \n",
       "128        0.0533         0.878   0.619    0.1130  0.518039       0.1020   \n",
       "348        0.0689         0.556   0.538    0.1960  0.569858       0.0382   \n",
       "81         0.9530         0.660   0.306    0.3190 -1.300739       0.0296   \n",
       "290        0.0206         0.680   0.894    0.4620  0.756679       0.2550   \n",
       "\n",
       "        tempo  valence  artist_id  target  \n",
       "90  -0.933822    0.416      19989       0  \n",
       "92  -1.005683    0.577      51281       0  \n",
       "94   0.840950    0.147      51281       0  \n",
       "95   0.304184    0.274      37945       0  \n",
       "96   1.730278    0.384      52541       0  \n",
       "..        ...      ...        ...     ...  \n",
       "321 -1.278019    0.235      20414       1  \n",
       "128  0.549449    0.639      29175       1  \n",
       "348  0.828597    0.291      40507       1  \n",
       "81  -1.013342    0.491      63007       1  \n",
       "290 -0.931246    0.803      37480       1  \n",
       "\n",
       "[174 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_fav = df[df['target'] == 0]\n",
    "fav = df[df['target'] == 1].sample(n=100)\n",
    "\n",
    "# merge\n",
    "df = pd.concat([not_fav, fav])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__ (self, emb1, emb2, target_col):\n",
    "        self.emb1 = emb1.values.astype(np.int32)\n",
    "        self.emb2 = emb2.values.astype(np.float32)\n",
    "        self.targets = target_col.values.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.emb1[index]), torch.tensor(self.emb2[index]), torch.tensor(self.targets[index])\n",
    "\n",
    "ds = CustomDataset(df.iloc[:,-2], df.iloc[:,:-2], df.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(19989, dtype=torch.int32),\n",
       " tensor([ 0.0014,  0.5170,  0.8390,  0.1080,  1.2132,  0.0287, -0.9338,  0.4160]),\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_data_set, test_data_set = model_selection.train_test_split(\n",
    "ds,\n",
    "test_size = 0.2,\n",
    "shuffle = True\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "train_data_set,\n",
    "shuffle = True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "test_data_set,\n",
    "shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139, 35)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(train_data_loader.dataset)\n",
    "val_size = len(test_data_loader.dataset)\n",
    "train_size, val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=True, delta=0, path='model.pth', trace_func=print):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model, 'model.pth')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EmbeddingLayer(rated_embedding_matrix)\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "_ground_truth = []\n",
    "_predicted = []\n",
    "\n",
    "\n",
    "def fit(num_epochs, learning_rate, model, train_loader, val_loader):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = [] \n",
    "\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        correct = 0\n",
    "        \n",
    "        model.train()\n",
    "        for (data1, data2, targets) in tqdm.tqdm(train_loader, desc= \"Training\"):\n",
    "            data1 = data1.to(device)\n",
    "            data2 = data2.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            scores = model(data1,data2)\n",
    "\n",
    "            loss = criterion(scores, targets)\n",
    "            _ground_truth.extend(targets.cpu().detach().numpy())\n",
    "            _predicted.extend(scores.cpu().detach().numpy())\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_value_(model.parameters(), 0.1)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            correct += (scores.detach().round() == targets).sum().item()\n",
    "            train_losses.append(loss.item())\n",
    "    \n",
    "        \n",
    "        model.eval()\n",
    "        correct_ = 0\n",
    "        for data1,data2, target in tqdm.tqdm(val_loader, desc= \"Evaluation\"):\n",
    "          data1 = data1.to(device)\n",
    "          data2 = data2.to(device)\n",
    "          target = target.to(device)\n",
    "          output = model(data1,data2)            \n",
    "          loss = criterion(output, target)\n",
    "          valid_losses.append(loss.item())\n",
    "          correct_ += (output.detach().round() == target).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / train_size\n",
    "        val_accuracy = 100 * correct_ / val_size\n",
    "\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        print(f\"{epoch+1} || valid_loss: {valid_loss} | train_loss: {train_loss} | accuracy: {accuracy} | val_accuracy: {val_accuracy}\")\n",
    "        \n",
    "        early_stopping(valid_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    print(\"Checking accuracy on test data\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1,x2, y in tqdm.tqdm(loader):\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            scores = model(x1,x2)\n",
    "            num_correct += (scores.round() == y).sum().item()\n",
    "            num_samples += 1*x1.shape[0]\n",
    "\n",
    "        print(f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 314.76it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 379.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 || valid_loss: 0.2556014710239002 | train_loss: 0.2443479376731159 | accuracy: 57.55395683453237 | val_accuracy: 51.42857142857143\n",
      "Validation loss decreased (inf --> 0.255601).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 314.75it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 380.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 || valid_loss: 0.24658324824912206 | train_loss: 0.22492864273702926 | accuracy: 62.589928057553955 | val_accuracy: 51.42857142857143\n",
      "Validation loss decreased (0.255601 --> 0.246583).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 349.48it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 353.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 || valid_loss: 0.26970091148146563 | train_loss: 0.21306154696043017 | accuracy: 66.18705035971223 | val_accuracy: 57.142857142857146\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 344.63it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 381.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 || valid_loss: 0.2447544916932072 | train_loss: 0.20646711453634628 | accuracy: 69.06474820143885 | val_accuracy: 48.57142857142857\n",
      "Validation loss decreased (0.246583 --> 0.244754).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 347.92it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 385.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 || valid_loss: 0.24552388010280474 | train_loss: 0.18715422697325082 | accuracy: 70.50359712230215 | val_accuracy: 57.142857142857146\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 346.16it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 384.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 || valid_loss: 0.25971841056432043 | train_loss: 0.18319422153340612 | accuracy: 72.66187050359713 | val_accuracy: 54.285714285714285\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 344.93it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 381.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 || valid_loss: 0.2372151910460421 | train_loss: 0.17131553135021663 | accuracy: 74.82014388489209 | val_accuracy: 57.142857142857146\n",
      "Validation loss decreased (0.244754 --> 0.237215).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 346.31it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 383.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 || valid_loss: 0.28055203746605134 | train_loss: 0.15851049425026817 | accuracy: 76.2589928057554 | val_accuracy: 51.42857142857143\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 344.99it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 384.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 || valid_loss: 0.26876227758558735 | train_loss: 0.14450363562067153 | accuracy: 79.13669064748201 | val_accuracy: 54.285714285714285\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 339.64it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 375.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 || valid_loss: 0.32214527592628395 | train_loss: 0.13434453552216963 | accuracy: 79.85611510791367 | val_accuracy: 48.57142857142857\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 306.94it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 380.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 || valid_loss: 0.284918693945344 | train_loss: 0.12227038529420124 | accuracy: 86.33093525179856 | val_accuracy: 48.57142857142857\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 139/139 [00:00<00:00, 324.43it/s]\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 362.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 || valid_loss: 0.2931046973872331 | train_loss: 0.11197264280273636 | accuracy: 85.61151079136691 | val_accuracy: 51.42857142857143\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "epochs = 1000\n",
    "history = fit(epochs, lr, model, train_data_loader, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8173602146681966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve, roc_curve\n",
    "\n",
    "roc_auc_scores = roc_auc_score(np.array(_ground_truth), np.array(_predicted))\n",
    "print(roc_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, _ = roc_curve(_ground_truth,  _predicted)\n",
    "auc = roc_auc_score(_ground_truth, _predicted)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:00<00:00, 6301.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 20 / 35 with accuracy 57.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(test_data_loader, torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
